# Run installation checks

name: pca-check

on:
  push:
    branches:
      - main
    paths:
      - gwaspy/pca/**

jobs:
  install:

    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.6, 3.7]

    steps:
    - uses: actions/checkout@v2
      with:
        lfs: true
    - uses: actions/setup-java@v2
      with:
        distribution: 'adopt'
        java-version: '8'
    - uses: vemonet/setup-spark@v1
      with:
        spark-version: '3.0.2'
        hadoop-version: '3.2'

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v2
      with:
        python-version: ${{ matrix.python-version }}
    - name: Set up Apache Spark
        run: |
          spark-submit --conf spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        python -m pip install flake8 pytest
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    - name: Install GWASpy
      run: |
        python setup.py sdist
        pip install dist/GWASpy-0.1.0.tar.gz
    - name: Checkout LFS objects
      run: git lfs checkout
    - name: PCA Run Test
      run: |
        gunzip -c data/1kg_annotated.mt/metadata.json.gz
        pca --dirname data/ --basename 1kg_annotated --out-dir data/ --input-type hail --reference grch37
